{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/uni_syd_logo.jpg\" width=\"200\" height=\"200\" style=\"float: right\" />\n",
    "# Feed Forward Neural Networks\n",
    "Dr. Rohitash Chandra and Rafael Possas - Centre of Translational Data Science / Sydney Informatics Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jdc as jdc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, layers, x_train, y_train, epochs, learning_rate, MinPer):\n",
    "        self.layers = layers  # NN topology [input, hidden, output]\n",
    "        self.epochs = epochs  # max epocs or training time\n",
    "        self.num_samples = len(x_train)\n",
    "\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "        self.learning_rate = learning_rate  # will be updated later with BP call\n",
    "        self.minPerf = MinPer\n",
    "        # initialize weights ( W1 W2 ) and bias ( b1 b2 ) of the network\n",
    "        np.random.seed()\n",
    "\n",
    "        self.w1 = np.random.randn(self.layers[0], self.layers[1])\n",
    "        self.b1 = np.random.randn(self.layers[1])  # bias first layer\n",
    "        print self.w1\n",
    "        self.best_b1 = self.b1\n",
    "        self.best_w1 = self.w1\n",
    "\n",
    "        self.w2 = np.random.randn(self.layers[1], self.layers[2])\n",
    "        self.b2 = np.random.randn(self.layers[2])  # bias second layer\n",
    "        self.best_b2 = self.b2\n",
    "        self.best_w2 = self.w2\n",
    "\n",
    "        self.hidden_out = np.zeros(self.layers[1])  # output of first hidden layer\n",
    "        self.hidden_delta = np.zeros(self.layers[1])  # gradient of first hidden layer\n",
    "\n",
    "        self.output = np.zeros(self.layers[2])  # output last (output) layer\n",
    "        self.output_delta = np.zeros(self.layers[2])  # gradient of  output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Perceptron model - Where it all started\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz0.png\" width=\"250\" height=\"250\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inputs x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>n</sub>\n",
    "- Outputs are computed using weights w<sub>1</sub>,w<sub>2</sub>, ...,w<sub>n</sub> that expressing the respective importance of the inputs to outputs\n",
    "- The neuron's output, 0 or 1, is determined by whether the weighted sum ∑<sub>j</sub>w<sub>j</sub>x<sub>j</sub> is less than or greater than some threshold value\n",
    "<img src=\"img/perceptron_outputs.png\" width=\"400\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How a Multi-layer perceptron would look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/nn_architecture.png\" width=\"500\" height=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First column of perceptrons is making three simple decisions by weighing the input evidence\n",
    "- Perceptrons on the second layer are making decisions by weighing at more abstract and complex level when compared to the first layer\n",
    "- This pattern repeats the deeper you get in the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron Example: Sigmoid Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz8.png\" width=\"450\" height=\"450\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Instead of using a linear (weighted) function we could use the sigmoid function that is pretty much a step function that shrinks all the values between 0 and 1, each Neuron would activate the closer the resulting value of the sigmoid is to 1\n",
    "- To put it all a little more explicitly, the output of a sigmoid neuron with inputs x1,x2,..., weights w1,w2,..., and bias b is\n",
    "<img src=\"img/sigmoid_plot.png\" width=\"450\" height=\"450\"/>\n",
    "<img src=\"img/sigmoid_neuron.png\" width=\"250\" height=\"250\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The result of the sigmoid function for each neuron gives its **activation** value, from now on denoted as **a**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def sigmoid(self, x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The smoothness of σ means that small changes Δwj in the weights and Δb in the bias will produce a small change Δoutput in the output from the neuron. In fact, calculus tells us that Δoutput is well approximated by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/output_delta.png\" width=\"350\" height=\"350\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we'd like is an algorithm which lets us find weights and biases so that the output from the network approximates **y(x)** for all training inputs **x**. To quantify how well we're achieving this goal we define a cost function: \n",
    "<img src=\"img/cost_function.png\" width=\"350\" height=\"350\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def calculate_error(self, actual_out, output_layer_size=None):\n",
    "    if output_layer_size is None:\n",
    "        output_layer_size = self.layers[-1]\n",
    "\n",
    "    error = np.sum(np.square(np.subtract(self.output, actual_out))) / output_layer_size\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find the minimum of such functions so as to we have the smaller difference between our predicted and real value. This can be done with the help of calculus and the **Gradient Descent Algorithm**\n",
    "<img src=\"img/valley_with_ball.png\" width=\"350\" height=\"350\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we move the ball in two different directions at a time, the total **cost change** can be found by using the following formula:\n",
    "<img src=\"img/gradient_desc.png\" width=\"300\" height=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the cost function **C** is the vector of partial derivatives for all variables, and can be defined by the formula: (Supposing we have only two variables, would work for any number of variables though)\n",
    "<img src=\"img/gradient_desc_2.png\" width=\"250\" height=\"250\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these definition we can then relate the changes in our **Cost Function** with the gradient vector and the changes in our values **v**, the following equation then can be defined:\n",
    "<img src=\"img/gradient_desc_3.png\" width=\"200\" height=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interesting thing about the equation is that it lets us see how to choose Δv so as to make ΔC negative. In particular, suppose we choose where η is a small, positive parameter (known as the learning rate), the ball will slowly go downhill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/gradient_desc_4.png\" width=\"200\" height=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the aforementioned definition we can then define the rules for updating our network **weights** and **biases** in order to decrease our cost function\n",
    "<img src=\"img/update_rule.png\" width=\"250\" height=\"250\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example code that calculates the minimum of a function with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The local minimum occurs at 2.249965\n"
     ]
    }
   ],
   "source": [
    "# From calculation, it is expected that the local minimum occurs at x=9/4\n",
    "\n",
    "cur_x = 6 # The algorithm starts at x=6\n",
    "gamma = 0.01 # step size multiplier\n",
    "precision = 0.00001\n",
    "previous_step_size = cur_x\n",
    "\n",
    "def df(x):\n",
    "    return 4 * x**3 - 9 * x**2\n",
    "\n",
    "while previous_step_size > precision:\n",
    "    prev_x = cur_x\n",
    "    cur_x += -gamma * df(prev_x)\n",
    "    previous_step_size = abs(cur_x - prev_x)\n",
    "\n",
    "print(\"The local minimum occurs at %f\" % cur_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With this new concept we can then define the backward pass for our network that updates our weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def update_weights_bias(self, weights, bias, hidden_layer_size, hidden_output, next_layer_size, next_layer_delta):\n",
    "    for x in range(0, hidden_layer_size):\n",
    "        for y in range(0, next_layer_size):\n",
    "            weights[x, y] += self.learning_rate * next_layer_delta[y] * hidden_output[x]\n",
    "\n",
    "    for y in range(0, next_layer_size):\n",
    "        bias[y] += -1 * self.learning_rate * next_layer_delta[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def backward_pass(self, input, y):\n",
    "\n",
    "    input_layer_size = self.layers[0]\n",
    "    hidden_layer_size = self.layers[1]\n",
    "    output_layer_size = self.layers[2]\n",
    "\n",
    "    self.output_delta = self.get_output_layer_gradient(y, output_layer_size)\n",
    "    self.hidden_delta = self.get_hidden_layer_gradient(hidden_layer_size,\n",
    "                                                       self.hidden_out, output_layer_size,\n",
    "                                                       self.output_delta, self.w2)\n",
    "\n",
    "    self.update_weights_bias(self.w2, self.b2, hidden_layer_size, self.hidden_out, output_layer_size,\n",
    "                             self.output_delta)\n",
    "\n",
    "    self.update_weights_bias(self.w1, self.b1, input_layer_size, input, hidden_layer_size,\n",
    "                             self.hidden_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given inputs x1,x2,..., xn, the outcome of the last layer is just the weigthed sum between all the layers in the network, where the output of one layer, is the input of the next layer. For our Network with one hidden layer here is the code that performs the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def get_weighted_sum(self,weights, bias, input_layer_size,input, output_layer_size, output, activation_function):\n",
    "    weighted_sum = 0\n",
    "    for y in range(0, output_layer_size):\n",
    "        for x in range(0, input_layer_size):\n",
    "            weighted_sum += input[x] * weights[x][y]\n",
    "            output[y] = activation_function(weighted_sum - bias[y])\n",
    "        weighted_sum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def forward_pass(self, input):\n",
    "\n",
    "    # This code expects a single hidden layer perceptron with input - hidden - output architecture\n",
    "    # THIS SHOULD CHANGE WHEN IMPLEMENTING MLP WITH MULTIPLE LAYERS\n",
    "\n",
    "    input_layer_size = self.layers[0]\n",
    "    hidden_layer_size = self.layers[1]\n",
    "    output_layer_size = self.layers[2]\n",
    "\n",
    "    # Each Neuron on the INPUT layer has 'y' connections to the neurons on the HIDDEN layer\n",
    "    # That's why the loop runs from the hidden layer to the input layer\n",
    "    self.get_weighted_sum(self.w1, self.b1, input_layer_size, input, hidden_layer_size,\n",
    "                          self.hidden_out, self.sigmoid)\n",
    "\n",
    "    # Each Neuron on the HIDDEN layer has 'y' connections to the neurons on the OUTPUT layer\n",
    "    # That's why the loop runs from the OUTPUT layer to the HIDDEN layer\n",
    "    self.get_weighted_sum(self.w2, self.b2, hidden_layer_size, self.hidden_out, output_layer_size,\n",
    "                          self.output, self.sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the notation easier we will now denote the weighted sum as **z**. For instance, **z<sup>l</sup>** would be the values for the layer **l**\n",
    "<img src=\"img/fwd_pass.png\" width=\"250\" height=\"250\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of backpropagation is to compute the partial derivatives ∂C/∂w and ∂C/∂b of the cost function **C** with respect to any weight **w** or bias **b** in the network. For backpropagation to work we need to make two main assumptions about the form of the cost function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The cost function can be written as an average C=1/n∑<sub>x</sub>C<sub>x</sub> over cost functions C<sub>x</sub> for individual training examples, **x**\n",
    "- The cost is that it can be written as a function of the outputs from the neural network\n",
    "<img src=\"img/backprop_assumption2.png\" width=\"450\" height=\"450\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going deeper into the backpropagation maths, lets define some notation:\n",
    "- Let's begin with a notation which lets us refer to weights in the network in an unambiguous way. We'll use **w<sup>l</sup><sub>jk</sub>** to denote the weight for the connection from the kthkth neuron in the **(l−1)th layer** to the **jth **neuron in the **lth layer**. So, for example, the diagram below shows the weight on a connection from the fourth neuron in the second layer to the second neuron in the third layer of a network:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"img/nn_notation.png\" width=\"450\" height=\"450\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We use a similar notation for the network's biases and activations. Explicitly, we use **b<sup>l</sup><sub>j</sub>** for the bias of the **jth** neuron in the **lth** layer. And we use **a<sup>l</sup><sub>j</sub>** for the activation of the **jth** neuron in the **lth** layer. The following diagram shows examples of these notations in use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/nn_notation_2.png\" width=\"350\" height=\"350\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With these notations, the activation **a<sup>l</sup><sub>j</sub>** of the **jth** neuron in the **lth** layer is related to the activations in the **(l−1)th** layer by the equation \n",
    "<img src=\"img/activation_next_layer.png\" width=\"300\" height=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagation** is based around four fundamental equations. Together, those equations give us a way of computing both the error δ<sup>l</sup> and the gradient of the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BP1: An equation for the error in the output layer, δ<sup>L</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/bp1_1.png\" width=\"200\" height=\"200\"/>\n",
    "<img src=\"img/bp1_2.png\" width=\"200\" height=\"200\"/>\n",
    "<img src=\"img/bp1_3.png\" width=\"200\" height=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def get_output_layer_gradient(self, y, layer_size):\n",
    "    \"\"\"\n",
    "    Compute gradients for the output layer which is defined by:\n",
    "         (gradient of the cost w.r.t neuron activation) * (derivative of the sigmoid of the neuron weighted sum)\n",
    "\n",
    "    It can bet interpreted as how much the cost and the activation is changing w.r.t to that neuron output\n",
    "    :param y: the real output\n",
    "    :param layer_size: the number of the neurons in the layer\n",
    "    :return: a delta array for that layer\n",
    "    \"\"\"\n",
    "    delta = np.zeros(layer_size)\n",
    "    for x in range(0, layer_size):\n",
    "        delta[x] = (y[x] - self.output[x]) * (self.output[x] * (1 - self.output[x]))\n",
    "\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BP2: An equation for the error δ<sup>L</sup> in terms of the error in the next layer\n",
    "<img src=\"img/bp2.png\" width=\"300\" height=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def get_hidden_layer_gradient(self, hidden_layer_size, hidden_output, next_layer_size, next_layer_delta, weights):\n",
    "    \"\"\"\n",
    "    Compute the error of the layer l w.r.t to layer l+1\n",
    "\n",
    "    It can bet interpreted as how much the cost and the activation is changing w.r.t to that neuron output\n",
    "    :param y: the real output\n",
    "    :param layer_size: the number of the neurons in the layer\n",
    "    :return: a delta array for that layer\n",
    "    \"\"\"\n",
    "    temp = 0\n",
    "    hidden_delta = np.zeros(hidden_layer_size)\n",
    "    for x in range(0, hidden_layer_size):\n",
    "        for y in range(0, next_layer_size):\n",
    "            temp += (next_layer_delta[y] * weights[x, y])\n",
    "            hidden_delta[x] = (hidden_output[x] * (1 - hidden_output[x])) * temp\n",
    "            temp = 0\n",
    "    return hidden_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By combining **BP2** and **BP1** we can compute the error for any layer in the network, we start by using the first equation to compute the delta in the output layer then we use equation 2 to compute in the other layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BP3: An equation for the rate of change of the cost with respect to any bias in the network\n",
    "<img src=\"img/bp3.png\" width=\"100\" height=\"100\"/>\n",
    "The error is exactly equal to the rate of change ∂C/∂b<sup>l</sup><sub>j</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BP4: An equation for the rate of change of the cost with respect to any weight in the network\n",
    "<img src=\"img/bp4.png\" width=\"150\" height=\"150\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us how to compute the partial derivatives ∂C/∂w<sup>l</sup><sub>jk</sub> in terms of the quantities δl and a<sup>l−1</sup>, which we already know how to compute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/bp_summary.png\" width=\"450\" height=\"450\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The full backproagation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def backpropagation(self, stochastic=True, train_tolerance=0):  # BP with Vanilla or SGD (Stocastic BP)\n",
    "\n",
    "    error = []  # np.zeros((1, self.Max))\n",
    "    epoch = 0\n",
    "    best_mean_squared_error = 100\n",
    "    bestTrain = 0\n",
    "\n",
    "    input_layer_size = self.layers[0]\n",
    "\n",
    "    while epoch < self.epochs and bestTrain < self.minPerf:\n",
    "\n",
    "        sse = 0\n",
    "        for s in range(0, self.num_samples):\n",
    "\n",
    "            if stochastic:\n",
    "                idx = random.randint(0, self.num_samples - 1)\n",
    "            else:\n",
    "                idx = s\n",
    "\n",
    "            # Get the first 'input_layer_size' columns as features\n",
    "            x = self.x_train[idx, 0:input_layer_size]\n",
    "            # Get the last 'output_layer_size' columns (real outputs)\n",
    "            y = self.x_train[idx, input_layer_size:]\n",
    "\n",
    "            self.forward_pass(x)\n",
    "            self.backward_pass(x, y)\n",
    "            sse = sse + self.calculate_error(y)\n",
    "\n",
    "        mean_squared_error = np.sqrt(sse / self.num_samples * self.layers[-1])\n",
    "\n",
    "\n",
    "        if mean_squared_error < best_mean_squared_error:\n",
    "            best_mean_squared_error = mean_squared_error\n",
    "            self.save_model()\n",
    "            (x, bestTrain) = self.TestNetwork(self.x_train, train_tolerance)\n",
    "\n",
    "        error = np.append(error, mean_squared_error)\n",
    "\n",
    "        epoch = epoch + 1\n",
    "\n",
    "\n",
    "    return (error, best_mean_squared_error, bestTrain, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def save_model(self):\n",
    "    self.best_w1 = self.w1\n",
    "    self.best_w2 = self.w2\n",
    "    self.best_b1 = self.b1\n",
    "    self.best_b2 = self.b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def TestNetwork(self, Data, erTolerance):\n",
    "\n",
    "    clasPerf = 0\n",
    "    sse = 0\n",
    "    self.w1 = self.best_w1\n",
    "    self.w2 = self.best_w2  # load best knowledge\n",
    "    self.b1 = self.best_b1\n",
    "    self.b2 = self.best_b2  # load best knowledge\n",
    "\n",
    "    testSize = Data.shape[0]\n",
    "\n",
    "    for s in range(0, testSize):\n",
    "\n",
    "        Input = Data[s, 0:self.layers[0]]\n",
    "        Desired = Data[s, self.layers[0]:]\n",
    "\n",
    "        self.forward_pass(Input)\n",
    "        sse = sse + self.calculate_error(Desired)\n",
    "\n",
    "        if (np.isclose(self.output, Desired, atol=erTolerance).any()):\n",
    "            clasPerf = clasPerf + 1\n",
    "\n",
    "    return (sse / testSize, float(clasPerf) / testSize * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalisedata(data, inputsize, outsize):  # normalise the data between [0,1]. This is important for most problems.\n",
    "    traindt = data[:, np.array(range(0, inputsize))]\n",
    "    dt = np.amax(traindt, axis=0)\n",
    "    tds = abs(traindt / dt)\n",
    "    return np.concatenate((tds[:, range(0, inputsize)], data[:, range(inputsize, inputsize + outsize)]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Network on some real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "problem = 1  # [1,2,3] choose your problem (Iris classfication or 4-bit parity or XOR gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.60759494  0.77272727  0.23188406  0.08        1.          0.        ]\n",
      " [ 0.60759494  0.68181818  0.20289855  0.04        1.          0.        ]\n",
      " [ 0.5443038   0.68181818  0.15942029  0.04        1.          0.        ]\n",
      " [ 0.73417722  0.90909091  0.17391304  0.08        1.          0.        ]\n",
      " [ 0.72151899  1.          0.2173913   0.16        1.          0.        ]\n",
      " [ 0.6835443   0.88636364  0.1884058   0.16        1.          0.        ]\n",
      " [ 0.64556962  0.79545455  0.20289855  0.12        1.          0.        ]\n",
      " [ 0.72151899  0.86363636  0.24637681  0.12        1.          0.        ]\n",
      " [ 0.93670886  0.63636364  0.88405797  0.76        0.          0.        ]\n",
      " [ 1.          0.86363636  0.92753623  0.8         0.          0.        ]\n",
      " [ 0.81012658  0.63636364  0.8115942   0.88        0.          0.        ]\n",
      " [ 0.79746835  0.63636364  0.73913043  0.6         0.          0.        ]\n",
      " [ 0.7721519   0.59090909  0.8115942   0.56        0.          0.        ]\n",
      " [ 0.97468354  0.68181818  0.88405797  0.92        0.          0.        ]\n",
      " [ 0.79746835  0.77272727  0.8115942   0.96        0.          0.        ]\n",
      " [ 0.81012658  0.70454545  0.79710145  0.72        0.          0.        ]\n",
      " [ 0.75949367  0.68181818  0.69565217  0.72        0.          0.        ]\n",
      " [ 0.87341772  0.70454545  0.7826087   0.84        0.          0.        ]\n",
      " [ 0.84810127  0.70454545  0.8115942   0.96        0.          0.        ]\n",
      " [ 0.64556962  0.86363636  0.2173913   0.12        1.          0.        ]\n",
      " [ 0.6835443   0.77272727  0.24637681  0.08        1.          0.        ]\n",
      " [ 0.64556962  0.84090909  0.2173913   0.16        1.          0.        ]\n",
      " [ 0.58227848  0.81818182  0.14492754  0.08        1.          0.        ]\n",
      " [ 0.64556962  0.75        0.24637681  0.2         1.          0.        ]\n",
      " [ 0.60759494  0.77272727  0.27536232  0.08        1.          0.        ]\n",
      " [ 0.63291139  0.68181818  0.23188406  0.08        1.          0.        ]\n",
      " [ 0.63291139  0.77272727  0.23188406  0.16        1.          0.        ]\n",
      " [ 0.65822785  0.79545455  0.2173913   0.08        1.          0.        ]\n",
      " [ 0.70886076  0.68181818  0.5942029   0.52        1.          1.        ]\n",
      " [ 0.69620253  0.56818182  0.57971014  0.52        1.          1.        ]\n",
      " [ 0.69620253  0.59090909  0.63768116  0.48        1.          1.        ]\n",
      " [ 0.7721519   0.68181818  0.66666667  0.56        1.          1.        ]\n",
      " [ 0.73417722  0.59090909  0.57971014  0.48        1.          1.        ]\n",
      " [ 0.63291139  0.52272727  0.47826087  0.4         1.          1.        ]\n",
      " [ 0.70886076  0.61363636  0.60869565  0.52        1.          1.        ]\n",
      " [ 0.72151899  0.68181818  0.60869565  0.48        1.          1.        ]\n",
      " [ 0.72151899  0.65909091  0.60869565  0.52        1.          1.        ]\n",
      " [ 0.78481013  0.65909091  0.62318841  0.52        1.          1.        ]\n",
      " [ 0.64556962  0.56818182  0.43478261  0.44        1.          1.        ]\n",
      " [ 0.72151899  0.63636364  0.5942029   0.52        1.          1.        ]\n",
      " [ 0.79746835  0.75        0.86956522  1.          0.          0.        ]\n",
      " [ 0.73417722  0.61363636  0.73913043  0.76        0.          0.        ]\n",
      " [ 0.89873418  0.68181818  0.85507246  0.84        0.          0.        ]\n",
      " [ 0.79746835  0.65909091  0.8115942   0.72        0.          0.        ]\n",
      " [ 0.82278481  0.68181818  0.84057971  0.88        0.          0.        ]\n",
      " [ 0.96202532  0.68181818  0.95652174  0.84        0.          0.        ]\n",
      " [ 0.62025316  0.56818182  0.65217391  0.68        0.          0.        ]\n",
      " [ 0.92405063  0.65909091  0.91304348  0.72        0.          0.        ]\n",
      " [ 0.84810127  0.56818182  0.84057971  0.72        0.          0.        ]\n",
      " [ 0.91139241  0.81818182  0.88405797  1.          0.          0.        ]\n",
      " [ 0.82278481  0.72727273  0.73913043  0.8         0.          0.        ]\n",
      " [ 0.81012658  0.61363636  0.76811594  0.76        0.          0.        ]\n",
      " [ 0.86075949  0.68181818  0.79710145  0.84        0.          0.        ]\n",
      " [ 0.72151899  0.56818182  0.72463768  0.8         0.          0.        ]\n",
      " [ 0.73417722  0.63636364  0.73913043  0.96        0.          0.        ]\n",
      " [ 0.81012658  0.72727273  0.76811594  0.92        0.          0.        ]\n",
      " [ 0.82278481  0.68181818  0.79710145  0.72        0.          0.        ]\n",
      " [ 0.97468354  0.86363636  0.97101449  0.88        0.          0.        ]\n",
      " [ 0.97468354  0.59090909  1.          0.92        0.          0.        ]\n",
      " [ 0.75949367  0.5         0.72463768  0.6         0.          0.        ]\n",
      " [ 0.87341772  0.72727273  0.82608696  0.92        0.          0.        ]\n",
      " [ 0.70886076  0.63636364  0.71014493  0.8         0.          0.        ]\n",
      " [ 0.65822785  0.77272727  0.20289855  0.08        1.          0.        ]\n",
      " [ 0.59493671  0.72727273  0.23188406  0.08        1.          0.        ]\n",
      " [ 0.60759494  0.70454545  0.23188406  0.08        1.          0.        ]\n",
      " [ 0.6835443   0.77272727  0.2173913   0.16        1.          0.        ]\n",
      " [ 0.65822785  0.93181818  0.2173913   0.04        1.          0.        ]\n",
      " [ 0.69620253  0.95454545  0.20289855  0.08        1.          0.        ]\n",
      " [ 0.62025316  0.70454545  0.2173913   0.04        1.          0.        ]\n",
      " [ 0.63291139  0.72727273  0.17391304  0.08        1.          0.        ]\n",
      " [ 0.69620253  0.79545455  0.1884058   0.08        1.          0.        ]\n",
      " [ 0.62025316  0.70454545  0.2173913   0.04        1.          0.        ]\n",
      " [ 0.55696203  0.68181818  0.1884058   0.08        1.          0.        ]\n",
      " [ 0.64556962  0.77272727  0.2173913   0.08        1.          0.        ]\n",
      " [ 0.63291139  0.79545455  0.1884058   0.12        1.          0.        ]\n",
      " [ 0.56962025  0.52272727  0.1884058   0.12        1.          0.        ]\n",
      " [ 0.55696203  0.72727273  0.1884058   0.08        1.          0.        ]\n",
      " [ 0.63291139  0.79545455  0.23188406  0.24        1.          0.        ]\n",
      " [ 0.64556962  0.86363636  0.27536232  0.16        1.          0.        ]\n",
      " [ 0.60759494  0.68181818  0.20289855  0.12        1.          0.        ]\n",
      " [ 0.64556962  0.86363636  0.23188406  0.08        1.          0.        ]\n",
      " [ 0.58227848  0.72727273  0.20289855  0.08        1.          0.        ]\n",
      " [ 0.67088608  0.84090909  0.2173913   0.08        1.          0.        ]\n",
      " [ 0.63291139  0.75        0.20289855  0.08        1.          0.        ]\n",
      " [ 0.88607595  0.72727273  0.68115942  0.56        1.          0.        ]\n",
      " [ 0.81012658  0.72727273  0.65217391  0.6         1.          1.        ]\n",
      " [ 0.87341772  0.70454545  0.71014493  0.6         1.          1.        ]\n",
      " [ 0.69620253  0.52272727  0.57971014  0.52        1.          1.        ]\n",
      " [ 0.82278481  0.63636364  0.66666667  0.6         1.          1.        ]\n",
      " [ 0.72151899  0.63636364  0.65217391  0.52        1.          1.        ]\n",
      " [ 0.79746835  0.75        0.68115942  0.64        1.          1.        ]\n",
      " [ 0.62025316  0.54545455  0.47826087  0.4         1.          1.        ]\n",
      " [ 0.83544304  0.65909091  0.66666667  0.52        1.          1.        ]\n",
      " [ 0.65822785  0.61363636  0.56521739  0.56        1.          1.        ]\n",
      " [ 0.63291139  0.45454545  0.50724638  0.4         1.          1.        ]\n",
      " [ 0.74683544  0.68181818  0.60869565  0.6         1.          1.        ]\n",
      " [ 0.75949367  0.5         0.57971014  0.4         1.          1.        ]\n",
      " [ 0.7721519   0.65909091  0.68115942  0.56        1.          1.        ]\n",
      " [ 0.70886076  0.65909091  0.52173913  0.52        1.          1.        ]\n",
      " [ 0.84810127  0.70454545  0.63768116  0.56        1.          1.        ]\n",
      " [ 0.70886076  0.68181818  0.65217391  0.6         1.          1.        ]\n",
      " [ 0.73417722  0.61363636  0.5942029   0.4         1.          1.        ]\n",
      " [ 0.78481013  0.5         0.65217391  0.6         1.          1.        ]\n",
      " [ 0.70886076  0.56818182  0.56521739  0.44        1.          1.        ]\n",
      " [ 0.74683544  0.72727273  0.69565217  0.72        1.          1.        ]\n",
      " [ 0.7721519   0.63636364  0.57971014  0.52        1.          1.        ]\n",
      " [ 0.79746835  0.56818182  0.71014493  0.6         1.          1.        ]\n",
      " [ 0.7721519   0.63636364  0.68115942  0.48        1.          1.        ]\n",
      " [ 0.81012658  0.65909091  0.62318841  0.52        1.          1.        ]\n",
      " [ 0.83544304  0.68181818  0.63768116  0.56        1.          1.        ]]\n",
      "printed data. now we use FNN for training ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'is the experimental run')\n",
      "[[-1.4656663  -1.28156284 -0.13902236  0.12963757 -0.30165471 -0.25532844]\n",
      " [ 0.01401649  1.04017474  0.66981064  0.11318718  0.08029558  1.7392951 ]\n",
      " [-0.70005952 -0.96010928  0.4336147   0.06950508  0.38317813  0.19480188]\n",
      " [ 1.46278207 -0.13848099 -0.40226704 -0.73850651  0.21523912  0.40342402]]\n",
      "(1, 'is the experimental run')\n",
      "[[-0.54419689 -0.34776485  0.22069437 -0.23283379  1.26696771  1.69250134]\n",
      " [ 0.27819473 -0.51765602 -0.88865629 -1.52007385 -2.43727243 -1.14104314]\n",
      " [-0.59179091  0.12220327  1.06090495 -0.62040567  0.45807473 -1.64548022]\n",
      " [-0.64803422 -0.26542509  1.30040473  1.10313542  0.20440772  1.31024431]]\n",
      "(2, 'is the experimental run')\n",
      "[[ 0.44424769 -0.98348218 -1.45056325 -0.85661157 -1.6752852   0.11488022]\n",
      " [-0.3343766  -0.30128574 -0.58522429 -0.88531165  0.58409244  0.32331694]\n",
      " [-1.19146305  1.25479727 -0.21952927 -1.30437513  0.73152064  0.87585057]\n",
      " [-0.51402173  0.33271075  2.94205476 -0.17446582  1.75313871 -0.1340396 ]]\n",
      "(3, 'is the experimental run')\n",
      "[[ 0.87602199  1.14360095 -1.32408205  0.7310455   0.1770479  -0.28322889]\n",
      " [-0.32242856 -2.68640288  0.89637184 -0.4197533  -0.95598687 -1.76294743]\n",
      " [-0.90314882  1.82781778  1.01633844  0.08567236 -0.56878354 -0.45592872]\n",
      " [-0.95659638  0.02244603 -1.54723969  1.52495561  0.30885349  1.59816763]]\n",
      "(4, 'is the experimental run')\n",
      "[[ 0.60872558  0.07882612  0.59274728  0.53950496  0.30911759 -0.40882104]\n",
      " [-0.92683793  0.32451647 -0.88179983  0.82151927 -1.08688511 -1.43167641]\n",
      " [ 2.12449552  0.60064288  1.12025672 -0.21065546 -0.16771565  1.61231917]\n",
      " [-1.59402831  2.35263537  0.27553203 -1.68184387 -0.07039871  0.63396218]]\n",
      "(5, 'is the experimental run')\n",
      "[[-0.98853625 -1.23678472  0.64787425 -0.21190124 -0.59747208 -0.18962073]\n",
      " [ 1.49943909 -0.12758358 -0.27512117 -0.00705092 -0.0843195   0.06245837]\n",
      " [ 1.52806123  0.49686768  1.6685219   0.14356672 -0.14232425 -0.99245656]\n",
      " [ 0.29092382 -0.41644221 -1.33484032  0.54938063 -1.05114075 -0.46995927]]\n",
      "(6, 'is the experimental run')\n",
      "[[-2.20653687  1.6830634   0.64864109 -0.94076222  1.40952495 -0.76735963]\n",
      " [ 0.29639422  0.12236321 -1.01756945 -1.39201954 -0.20441753 -1.47159092]\n",
      " [-0.75779923  0.66081944  0.72463017 -1.22446279 -1.85141254 -0.48059438]\n",
      " [ 1.39905444 -0.43389068  1.02947927  0.5223207   0.93230013  0.72688944]]\n",
      "(7, 'is the experimental run')\n",
      "[[-0.71180492 -0.74611996 -0.32898844  0.43001852 -1.82105293 -1.42415213]\n",
      " [-2.30791048  1.25049221 -1.22142129 -0.8161763  -0.75468754  0.73931975]\n",
      " [ 0.18149276 -1.18290277 -0.16578198 -1.80595487  0.50813287 -0.4528364 ]\n",
      " [ 1.24456382 -0.18102166 -0.51592185 -1.27121747  1.64075539 -0.85599699]]\n",
      "(8, 'is the experimental run')\n",
      "[[ 1.26821901  1.02263731  0.01564214 -2.37134648 -0.22096917  2.30409534]\n",
      " [-1.28247891  1.82053299 -0.64971047  0.02482702  0.78088203 -0.23583781]\n",
      " [ 1.11603365  0.35541797  0.18296254  0.7092311  -0.53598634  0.56859984]\n",
      " [-0.00823905 -1.50317027  0.56634953  2.04610434 -0.22061541 -0.38307912]]\n",
      "(9, 'is the experimental run')\n",
      "[[ 0.02407206  0.39321403  1.45982491 -1.09766285  0.06378304  0.44518185]\n",
      " [-0.70941517  0.64582823  0.7948284  -2.26580711  1.05664493 -1.99033719]\n",
      " [-0.43479556 -0.77817379  0.13034286 -1.30919704 -0.00769098 -2.14173799]\n",
      " [ 0.3143114  -0.65362721 -0.88968366  0.74944883 -0.87247729 -0.46564854]]\n",
      "(array([  91.81818182,  100.        ,   92.72727273,   90.90909091,\n",
      "         91.81818182,   90.        ,   89.09090909,   92.72727273,\n",
      "         93.63636364,   91.81818182]), 'train perf % for n exp')\n",
      "(array([  95. ,  100. ,   95. ,  100. ,   97.5,   95. ,   95. ,   97.5,\n",
      "         97.5,   97.5]), 'test  perf % for n exp')\n",
      "(array([ 0.14112315,  0.74869164,  0.13420487,  0.15004781,  0.15147153,\n",
      "        0.15344045,  0.15377227,  0.12320347,  0.15148709,  0.13748431]), 'train mean squared error for n exp')\n",
      "(array([ 0.03049236,  0.27319428,  0.0356992 ,  0.03493938,  0.02865673,\n",
      "        0.03452919,  0.03554339,  0.03127857,  0.03661316,  0.0320002 ]), 'test mean squared error for n exp')\n",
      "mean and std for training perf %\n",
      "(92.454545454545453, 2.8181818181818175)\n",
      "mean and std for test perf %\n",
      "(97.0, 1.8708286933869707)\n",
      "mean and std for time in seconds\n",
      "(9.1047422885894775, 3.0254272271502494)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x108b48190>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD8CAYAAACYebj1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108b02f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if problem == 1:\n",
    "    TrDat = np.loadtxt(\"train.csv\", delimiter=',')  # Iris classification problem (UCI dataset)\n",
    "    TesDat = np.loadtxt(\"test.csv\", delimiter=',')  #\n",
    "    Hidden = 6\n",
    "    Input = 4\n",
    "    Output = 2\n",
    "    TrSamples = 110\n",
    "    TestSize = 40\n",
    "    learnRate = 0.1\n",
    "    TrainData = normalisedata(TrDat, Input, Output)\n",
    "    TestData = normalisedata(TesDat, Input, Output)\n",
    "    MaxTime = 500\n",
    "    MinCriteria = 95  # stop when learn 95 percent\n",
    "\n",
    "if problem == 2:\n",
    "    TrainData = np.loadtxt(\"4bit.csv\", delimiter=',')  # 4-bit parity problem\n",
    "    TestData = np.loadtxt(\"4bit.csv\", delimiter=',')  #\n",
    "    Hidden = 4\n",
    "    Input = 4\n",
    "    Output = 1\n",
    "    TrSamples = 16\n",
    "    TestSize = 16\n",
    "    learnRate = 0.9\n",
    "    MaxTime = 3000\n",
    "    MinCriteria = 95  # stop when learn 95 percent\n",
    "\n",
    "if problem == 3:\n",
    "    TrainData = np.loadtxt(\"xor.csv\", delimiter=',')  # 4-bit parity problem\n",
    "    TestData = np.loadtxt(\"xor.csv\", delimiter=',')  #\n",
    "    Hidden = 3\n",
    "    Input = 2\n",
    "    Output = 1\n",
    "    TrSamples = 4\n",
    "    TestSize = 4\n",
    "    learnRate = 0.9\n",
    "    MaxTime = 500\n",
    "    MinCriteria = 100  # stop when learn 95 percent\n",
    "\n",
    "print(TrainData)\n",
    "\n",
    "print('printed data. now we use FNN for training ...')\n",
    "\n",
    "layers = [Input, Hidden, Output]\n",
    "MaxRun = 10  # number of experimental runs\n",
    "\n",
    "trainTolerance = 0.2  # [eg 0.15 would be seen as 0] [ 0.81 would be seen as 1]\n",
    "testTolerance = 0.49\n",
    "\n",
    "trainPerf = np.zeros(MaxRun)\n",
    "testPerf = np.zeros(MaxRun)\n",
    "\n",
    "trainMSE = np.zeros(MaxRun)\n",
    "testMSE = np.zeros(MaxRun)\n",
    "Epochs = np.zeros(MaxRun)\n",
    "Time = np.zeros(MaxRun)\n",
    "\n",
    "stocastic = 1  # 0 if vanilla (batch mode)\n",
    "\n",
    "for run in range(0, MaxRun):\n",
    "    print(run, 'is the experimental run')\n",
    "    fnnSGD = Network(layers, TrainData, TestData, MaxTime, learnRate, MinCriteria)\n",
    "    start_time = time.time()\n",
    "    (erEp, trainMSE[run], trainPerf[run], Epochs[run]) = fnnSGD.backpropagation(stocastic, trainTolerance)\n",
    "    Time[run] = time.time() - start_time\n",
    "    (testMSE[run], testPerf[run]) = fnnSGD.TestNetwork(TestData, testTolerance)\n",
    "\n",
    "print(trainPerf, 'train perf % for n exp')\n",
    "print(testPerf, 'test  perf % for n exp')\n",
    "print(trainMSE, 'train mean squared error for n exp')\n",
    "print(testMSE, 'test mean squared error for n exp')\n",
    "\n",
    "print('mean and std for training perf %')\n",
    "print(np.mean(trainPerf), np.std(trainPerf))\n",
    "\n",
    "print('mean and std for test perf %')\n",
    "print(np.mean(testPerf), np.std(testPerf))\n",
    "\n",
    "print('mean and std for time in seconds')\n",
    "print(np.mean(Time), np.std(Time))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(erEp)\n",
    "plt.ylabel('error')\n",
    "#plt.savefig(str(problem) + 'out.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
